{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4\n",
    "!pip install sqlalchemy psycopg2-binary pandas\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books(base_url=\"http://books.toscrape.com/\", timeout=10, wait=1, save_json=True):\n",
    "\n",
    "    url = base_url + \"catalogue/page-1.html\"\n",
    "    libros = []\n",
    "    i = 1\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            print(f\"\\nAccediendo a: {url}\")\n",
    "\n",
    "            uClient = urlopen(url, timeout=timeout)\n",
    "            page_html = uClient.read()\n",
    "            uClient.close()\n",
    "\n",
    "            page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "            bookshelf = page_soup.findAll(\"li\", {\"class\": \"col-xs-6 col-sm-4 col-md-3 col-lg-3\"})\n",
    "\n",
    "            for book in bookshelf:\n",
    "                book_title = book.h3.a[\"title\"]\n",
    "                a_href = book.h3.a[\"href\"]\n",
    "                book_url = base_url + \"catalogue/\" + a_href.replace(\"../../../\", \"\")\n",
    "\n",
    "                libros.append({\n",
    "                    \"titulo\": book_title,\n",
    "                    \"url\": book_url\n",
    "                })\n",
    "\n",
    "                print(f\"{i}. {book_title}\")\n",
    "                i += 1\n",
    "\n",
    "            next_button = page_soup.find(\"li\", {\"class\": \"next\"})\n",
    "            if next_button:\n",
    "                next_href = next_button.a[\"href\"]\n",
    "                url = base_url + \"catalogue/\" + next_href\n",
    "            else:\n",
    "                url = None\n",
    "\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print(f\"Error HTTP {e.code} al acceder a {url}\")\n",
    "            break\n",
    "        except URLError as e:\n",
    "            print(f\"Error de conexión: {e.reason}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error inesperado: {e}\")\n",
    "            break\n",
    "\n",
    "    if save_json:\n",
    "        with open(\"libros_url.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(libros, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nTotal de libros scrapeados: {len(libros)}\")\n",
    "    return libros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamar a la función\n",
    "libros = scrape_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b080977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "url = []\n",
    "for libro in libros:\n",
    "    url.append(libro[\"url\"])\n",
    "print(len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def obtener_datos_libro(url_libro):\n",
    "    response = requests.get(url_libro)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Título\n",
    "    titulo = soup.h1.text.strip()\n",
    "\n",
    "    # Precio\n",
    "    precio_tag = soup.find('p', class_='price_color')\n",
    "    precio = precio_tag.text.strip()\n",
    "\n",
    "    # Rating\n",
    "    rating_tag = soup.find('p', class_='star-rating')\n",
    "    rating_str = rating_tag['class'][1]\n",
    "\n",
    "    rating_map = {\n",
    "        \"One\": 1,\n",
    "        \"Two\": 2,\n",
    "        \"Three\": 3,\n",
    "        \"Four\": 4,\n",
    "        \"Five\": 5\n",
    "    }\n",
    "\n",
    "    rating = rating_map.get(rating_str, 0) \n",
    "\n",
    "    # Descripción\n",
    "    descripcion_tag = soup.select_one('#product_description ~ p')\n",
    "    descripcion = descripcion_tag.text.strip() if descripcion_tag else \"\"\n",
    "\n",
    "    # Género (3er <li> en el breadcrumb)\n",
    "    breadcrumb = soup.select(\"ul.breadcrumb li a\")\n",
    "    genero = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
    "\n",
    "\n",
    "    # Tabla de datos adicionales\n",
    "    tabla = soup.find('table', class_='table table-striped')\n",
    "    filas = tabla.find_all('tr')\n",
    "    datos = {fila.th.text.strip(): fila.td.text.strip() for fila in filas}\n",
    "\n",
    "    # Diccionario con los datos\n",
    "    libro_data = {\n",
    "        \"Titulo\": titulo,\n",
    "        \"Precio\": precio,\n",
    "        \"rating\": rating,\n",
    "        \"descripcion\": descripcion,\n",
    "        \"Genero\": genero, \n",
    "        \"Upc\": datos.get('UPC', ''),\n",
    "        \"tipo_producto\": datos.get('Product Type', ''),\n",
    "        \"precio_excl_tax\": datos.get('Price (excl. tax)', ''),\n",
    "        \"precio_incl_tax\": datos.get('Price (incl. tax)', ''),\n",
    "        \"tax\": datos.get('Tax', ''),\n",
    "        \"numero_stock\": ''.join(filter(str.isdigit, datos.get('Availability', ''))),\n",
    "        \"numero_reviews\": datos.get('Number of reviews', ''),\n",
    "    }\n",
    "\n",
    "    return libro_data\n",
    "\n",
    "# ⬇️ Procesar todos los libros y guardar al final\n",
    "todos_los_libros = []\n",
    "i = 1\n",
    "for libro in libros:\n",
    "    print(f\"Libro scrapeado {i}\")\n",
    "    datos = obtener_datos_libro(libro[\"url\"])\n",
    "    todos_los_libros.append(datos)\n",
    "    i += 1\n",
    "\n",
    "# Guardar todo en un único JSON\n",
    "with open(\"libros.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(todos_los_libros, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ Total de libros guardados: {len(todos_los_libros)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_autores_google_books(archivo_in=\"libros.json\", archivo_out=\"autores_libros.json\"):\n",
    "    with open(archivo_in, \"r\", encoding=\"utf-8\") as f:\n",
    "        libros = json.load(f)\n",
    "\n",
    "    autores_data = []\n",
    "\n",
    "    for libro in libros:\n",
    "        query = libro[\"Titulo\"].replace(\" \", \"+\")  # limpiar búsqueda\n",
    "        url = f\"https://www.googleapis.com/books/v1/volumes?q=intitle:{query}\"\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=10).json()\n",
    "\n",
    "            autores = []\n",
    "            if \"items\" in resp:\n",
    "                volumen = resp[\"items\"][0][\"volumeInfo\"]\n",
    "                autores = volumen.get(\"authors\", [])\n",
    "\n",
    "            autores_data.append({\n",
    "                \"titulo\": libro[\"Titulo\"],\n",
    "                \"autores\": autores\n",
    "            })\n",
    "\n",
    "            print(f\"{libro['Titulo']} → {autores if autores else 'No encontrado'}\")\n",
    "\n",
    "            time.sleep(0.5)  # anti-rate-limit\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error con '{libro['Titulo']}': {e}\")\n",
    "            autores_data.append({\n",
    "                \"titulo\": libro[\"Titulo\"],\n",
    "                \"autores\": []\n",
    "            })\n",
    "\n",
    "    with open(archivo_out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(autores_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return autores_data\n",
    "\n",
    "\n",
    "# Uso:\n",
    "obtener_autores_google_books()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc3046",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
